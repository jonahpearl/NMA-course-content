{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0fa376",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/projects/fMRI/load_cichy_fMRI_MEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/projects/fMRI/load_cichy_fMRI_MEG.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af2aee",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137a296",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here we will load data from Cichy et al. 2014 [1]. The data consist of fMRI responses from early visual cortex (EVC) and inferior temporal (IT) cortex and MEG responses at different timepoints in the form of representational dissimilarity matrices (RDMs) to 92 images. These images belong to different categories as shown in the Figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e372d",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/course-content/blob/main/projects/fMRI/static/cichy_fMRI_MEG_fig1.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a9eaf",
   "metadata": {},
   "source": [
    "## Representational Similarity Analysis (RSA)\n",
    "\n",
    "RSA is a method to relate signals from different source spaces (such as behavior, neural responses, DNN activations) by abstracting signals from\n",
    "separate source spaces into a common similarity space. For this, in each source space, condition-specific responses are compared to each other for dissimilarity (e.g., by calculating Euclidean distances between signals), and the values are aggregated in so-called representational dissimilarity matrices (RDMs) indexed in rows and columns by the conditions compared. RDMs thus summarize the representational geometry of the source space signals. Different from source space signals themselves, RDMs from different sources spaces are directly comparable to each other for similarity and thus can relate signals from different spaces.\n",
    "\n",
    "The figure below illustrates how RSA can be applied to different problems by comparing RDMs of different modalities/species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6050c4",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/course-content/blob/main/projects/fMRI/static/cichy_fMRI_MEG_fig2.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820bb0e",
   "metadata": {},
   "source": [
    "## Data from Cichy et al., 2014\n",
    "\n",
    "In the cells below, we will download and visualize MEG and fMRI RDMs. Please refer Figure 1 in [1] to learn details about the image category order in RDMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1955ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import urllib\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd import Variable as V\n",
    "from torchvision import transforms as trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5529ed8",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Load mat files\n",
    "def loadmat(matfile):\n",
    "    \"\"\"Function to load .mat files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    matfile : str\n",
    "        path to `matfile` containing fMRI data for a given trial.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dictionary containing data in key 'vol' for a given trial.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = h5py.File(matfile)\n",
    "    except (IOError, OSError):\n",
    "        return sio.loadmat(matfile)\n",
    "    else:\n",
    "        return {name: np.transpose(f.get(name)) for name in f.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136549b",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Data download and unzip in `data/`\n",
    "\n",
    "import os, zipfile, requests\n",
    "\n",
    "fname = 'data.zip'\n",
    "url = \"https://osf.io/7vpyh/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            print(\"Downlading data...\")\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)\n",
    "            with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "                zip_ref.extractall('data/')\n",
    "            print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1a81c",
   "metadata": {},
   "source": [
    "## Loading MEG RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716232bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MEG RDMs for each time point for all subjects all sessions\n",
    "MEG_RDMs = loadmat(\"data/MEG_decoding_RDMs.mat\")['MEG_decoding_RDMs']\n",
    "print(MEG_RDMs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7987759",
   "metadata": {},
   "source": [
    "Shape of RDM is `num_subjects x num_sessions x num_timepoints x num_stimulus x num_stimulus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average RDM across subjects and sessions\n",
    "MEG_RDM_sub_averaged = np.mean(MEG_RDMs, axis=(0, 1))\n",
    "del MEG_RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a76eb",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title visualize subject averaged MEG RDMs\n",
    "timepoint = 420  # @param {type:\"slider\", min:-100, max:600, step:20}\n",
    "\n",
    "# Load RDM at a given timepoint\n",
    "# +100 as the RDMs provided are from -100ms to 1000ms after the stimulus onset\n",
    "RDM = np.array(MEG_RDM_sub_averaged[timepoint+100])\n",
    "\n",
    "# Since the matrix is symmetric we set upper triangular values to NaN\n",
    "RDM[np.triu_indices(RDM.shape[0], 1)] = np.nan\n",
    "\n",
    "# plot the RDM at given timepoint\n",
    "plt.imshow(RDM, cmap=\"bwr\")\n",
    "plt.title(\"MEG RDM at t = \" + str(timepoint))\n",
    "cbar = plt.colorbar()\n",
    "plt.xlabel(\"Stimuli\")\n",
    "plt.ylabel(\"Stimuli\")\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('Decoding Accuracy', rotation=270)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2df3fe",
   "metadata": {},
   "source": [
    "##Loading fMRI RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bf2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fMRI_file = 'data/92_Image_Set/target_fmri.mat'  # path of fMRI RDM file\n",
    "fMRI_RDMs = loadmat(fMRI_file)  # load the fMRI RDMs\n",
    "print(fMRI_RDMs.keys())\n",
    "print(fMRI_RDMs['EVC_RDMs'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b39b1b",
   "metadata": {},
   "source": [
    "fMRI_RDMs is a dictionary with keys `'EVC_RDMs'` and `'IT_RDMs'` corresponding to ROIs EVC and IT respectively. The shape of each RDM is `num_subjects` $\\times$ `num_stimulus` $\\times$ `num_stimulus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38149d07",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title visualize subject averaged fMRI RDMs\n",
    "ROI = 'EVC'  # @param [\"EVC\", \"IT\"]\n",
    "\n",
    "# Average the ROI RDM across subjects\n",
    "RDM = np.array(fMRI_RDMs[ROI + '_RDMs'].mean(axis=0))\n",
    "\n",
    "# Since the matrix is symmetric we set upper triangular values to NaN\n",
    "RDM[np.triu_indices(RDM.shape[0], 1)] = np.nan\n",
    "\n",
    "# plot the ROI RDM at given timepoint\n",
    "plt.imshow(RDM, cmap=\"bwr\")\n",
    "plt.title(ROI + \" RDM\")\n",
    "cbar = plt.colorbar()\n",
    "plt.xlabel(\"Stimuli\")\n",
    "plt.ylabel(\"Stimuli\")\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('1-Correlation', rotation=270)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c5725",
   "metadata": {},
   "source": [
    "# Example Analyses\n",
    "\n",
    "Below we will perform two analyses:\n",
    "\n",
    "1. MEG-fMRI comparison: To find out at which timepoint MEG representation is similar to a given ROI's representation.\n",
    "2. MEG-Deep Neural Network (DNN) comparison: To find out at which timepoint MEG representation is similar to a given DNN layer's representation.\n",
    "\n",
    "In other words, the comparison will inform us about the sequential order of visual feature processing in the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2e2f6",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title RDM Comparison functions\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def RSA_spearman(rdm1,rdm2):\n",
    "    \"\"\"\n",
    "    computes and returns the spearman correlation between lower triangular\n",
    "    part of the input rdms. We only need to compare either lower or upper\n",
    "    triangular part of the matrix as RDM is symmetric\n",
    "    \"\"\"\n",
    "    # get lower triangular part of the RDM1\n",
    "    lt_rdm1 = get_lowertriangular(rdm1)\n",
    "    # get lower triangular part of the RDM1\n",
    "    lt_rdm2 = get_lowertriangular(rdm2)\n",
    "    # return Spearman's correlation between lower triangular part of rdm1 & rdm2\n",
    "    return spearmanr(lt_rdm1, lt_rdm2)[0]\n",
    "\n",
    "def get_lowertriangular(rdm):\n",
    "    \"\"\"\n",
    "    returns lower triangular part of the matrix\n",
    "    \"\"\"\n",
    "    num_conditions = rdm.shape[0]\n",
    "    return rdm[np.tril_indices(num_conditions, -1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5078c0",
   "metadata": {},
   "source": [
    "##MEG-fMRI Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb090d2",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Correlating MEG RDMs with fMRI RDMs\n",
    "num_timepoints = MEG_RDM_sub_averaged.shape[0] #get number of timepoints\n",
    "\n",
    "# initialize a dictionary to store MEG and ROI RDM correlation at each timepoint\n",
    "MEG_correlation = {}\n",
    "ROIs = ['EVC','IT']\n",
    "for ROI in ROIs:\n",
    "    MEG_correlation[ROI] = []\n",
    "\n",
    "# for loop that goes over MEG RDMs at all time points and correlate with ROI RDMs\n",
    "for t in range(num_timepoints):\n",
    "    MEG_RDM_t = MEG_RDM_sub_averaged[t, :, :]\n",
    "    for ROI in ROIs:\n",
    "        ROI_RDM = np.mean(fMRI_RDMs[ROI + '_RDMs'], axis=0)\n",
    "        MEG_correlation[ROI].append(RSA_spearman(ROI_RDM, MEG_RDM_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0b277",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting MEG-fMRI comparison\n",
    "\n",
    "plt.rc('font', size=12)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "time_range = range(-100, 1201)\n",
    "ax.plot(time_range, MEG_correlation['IT'], color='tab:orange', label='IT')\n",
    "ax.plot(time_range, MEG_correlation['EVC'], color='tab:blue', label='EVC')\n",
    "\n",
    "# Same as above\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Spearmans Correlation')\n",
    "ax.set_title('MEG-fMRI fusion')\n",
    "ax.grid(True)\n",
    "ax.legend(loc='upper left')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b5dde",
   "metadata": {},
   "source": [
    "## MEG-DNN Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8997d2d2",
   "metadata": {},
   "source": [
    "### Creating DNN (AlexNet) RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463e35c",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title AlexNet Definition\n",
    "__all__ = ['AlexNet', 'alexnet']\n",
    "\n",
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "}\n",
    "\n",
    "# Here we redefine AlexNet differently from torchvision code for better understanding\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "        self.fc6 = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        self.fc7 =nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            )\n",
    "        self.fc8 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(out1)\n",
    "        out3 = self.conv3(out2)\n",
    "        out4 = self.conv4(out3)\n",
    "        out5 = self.conv5(out4)\n",
    "\n",
    "        out5_reshaped = out5.view(out5.size(0), 256 * 6 * 6)\n",
    "        out6= self.fc6(out5_reshaped)\n",
    "        out7= self.fc7(out6)\n",
    "        out8 = self.fc8(out7)\n",
    "        return out1, out2, out3,out4, out5, out6,out7,out8\n",
    "\n",
    "\n",
    "def alexnet(pretrained=False, **kwargs):\n",
    "    \"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcef52a",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Feature extraction code\n",
    "\n",
    "def load_alexnet(model_checkpoints):\n",
    "    \"\"\"This function initializes an Alexnet and load\n",
    "    its weights from a pretrained model. Since we redefined model in a different\n",
    "    way we have to rename the weights that were in the pretrained checkpoint.\n",
    "    ----------\n",
    "    model_checkpoints : str\n",
    "        model checkpoints location.\n",
    "    Returns\n",
    "    -------\n",
    "    model\n",
    "        pytorch model of alexnet\n",
    "    \"\"\"\n",
    "\n",
    "    model = alexnet()\n",
    "    # Load checkpoint\n",
    "    model_file = model_checkpoints\n",
    "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # Rename the checkpoint keys according to new definition\n",
    "    model_dict = [\"conv1.0.weight\", \"conv1.0.bias\",\n",
    "                  \"conv2.0.weight\", \"conv2.0.bias\",\n",
    "                  \"conv3.0.weight\", \"conv3.0.bias\",\n",
    "                  \"conv4.0.weight\", \"conv4.0.bias\",\n",
    "                  \"conv5.0.weight\", \"conv5.0.bias\",\n",
    "                  \"fc6.1.weight\", \"fc6.1.bias\",\n",
    "                  \"fc7.1.weight\", \"fc7.1.bias\",\n",
    "                  \"fc8.1.weight\", \"fc8.1.bias\"]\n",
    "    state_dict = {}\n",
    "    i = 0\n",
    "    for k, v in checkpoint.items():\n",
    "        state_dict[model_dict[i]] =  v\n",
    "        i += 1\n",
    "\n",
    "    # initialize model with pretrained weights\n",
    "    model.load_state_dict(state_dict)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_activations_and_save(model, image_list, activations_dir):\n",
    "    \"\"\"This function generates Alexnet features and save them in a specified directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        pytorch model : alexnet.\n",
    "    image_list : list\n",
    "        the list contains path to all images.\n",
    "    activations_dir : str\n",
    "        save path for extracted features.\n",
    "    \"\"\"\n",
    "\n",
    "    resize_normalize = trn.Compose([\n",
    "                                    trn.Resize((224, 224)),\n",
    "                                    trn.ToTensor(),\n",
    "                                    trn.Normalize([0.485, 0.456, 0.406],\n",
    "                                                  [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "    # for all images in the list generate and save activations\n",
    "    for image_file in tqdm(image_list):\n",
    "        # open image\n",
    "        img = Image.open(image_file)\n",
    "        image_file_name = os.path.split(image_file)[-1].split(\".\")[0]\n",
    "        # apply transformations before feeding to model\n",
    "        input_img = V(resize_normalize(img).unsqueeze(0))\n",
    "        if torch.cuda.is_available():\n",
    "            input_img=input_img.cuda()\n",
    "        x = model.forward(input_img)\n",
    "\n",
    "        activations = []\n",
    "        for i,feat in enumerate(x):\n",
    "            activations.append(feat.data.cpu().numpy().ravel())\n",
    "        for layer in range(len(activations)):\n",
    "            save_path = os.path.join(activations_dir,\n",
    "                                      f\"{image_file_name}_{layer}_{layer + 1}.npy\")\n",
    "            np.save(save_path, activations[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ebc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to all the images in the stimulus set\n",
    "image_dir = 'data/92_Image_Set/92images'\n",
    "image_list = glob.glob(image_dir + '/*.jpg')\n",
    "image_list.sort()\n",
    "print(f'Total Number of Images: {len(image_list)}')\n",
    "save_dir = \"activations_alexnet\"\n",
    "\n",
    "######### load Alexnet initialized with pretrained weights ###################\n",
    "# Download pretrained Alexnet from:\n",
    "# https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\n",
    "# and save in the current directory\n",
    "checkpoint_path = \"alexnet.pth\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    url = \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\"\n",
    "    urllib.request.urlretrieve(url, \"alexnet.pth\")\n",
    "model = load_alexnet(checkpoint_path)\n",
    "##############################################################################\n",
    "\n",
    "######### get and save activations ################################\n",
    "\n",
    "activations_dir = os.path.join(save_dir)\n",
    "if not os.path.exists(activations_dir):\n",
    "    os.makedirs(activations_dir)\n",
    "print(\"-------------Saving activations ----------------------------\")\n",
    "get_activations_and_save(model, image_list, activations_dir)\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8  # number of layers in the model\n",
    "layers = []\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layers.append(f\"layer_{i + 1}\")\n",
    "\n",
    "model_RDMs = {}\n",
    "# create RDM for each layer from activations\n",
    "for j, layer in enumerate(layers):\n",
    "    activation_files = glob.glob(activations_dir + '/*' + f'{j}.npy')\n",
    "    activation_files.sort()\n",
    "    activations = []\n",
    "    # Load all activations\n",
    "    for activation_file in activation_files:\n",
    "        activations.append(np.load(activation_file))\n",
    "    activations = np.array(activations)\n",
    "    # calculate Pearson's distance for all pairwise comparisons\n",
    "    model_RDMs[layer] = 1 - np.corrcoef(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2451194",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title visualize model RDMs\n",
    "layer = 'layer_8'  # @param ['layer_1','layer_2','layer_3','layer_4','layer_5','layer_6','layer_7','layer_8']\n",
    "\n",
    "# loading layer RDM\n",
    "RDM = np.array(model_RDMs[layer])\n",
    "\n",
    "# Since the matrix is symmetric we set upper triangular values to NaN\n",
    "RDM[np.triu_indices(RDM.shape[0], 1)] = np.nan\n",
    "\n",
    "# Visualize layer RDM\n",
    "plt.imshow(RDM, cmap=\"bwr\")\n",
    "plt.title(layer + \" RDM\")\n",
    "cbar = plt.colorbar()\n",
    "plt.xlabel(\"Stimuli\")\n",
    "plt.ylabel(\"Stimuli\")\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('1-Correlation', rotation=270)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48b884",
   "metadata": {},
   "source": [
    "### Comparing MEG RDMs with AlexNet RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5e2bd",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Correlating MEG RDMs with DNN RDMs\n",
    "run = False\n",
    "\n",
    "if run:\n",
    "    num_timepoints = MEG_RDM_sub_averaged.shape[0]  # get number of timepoints\n",
    "\n",
    "    # initialize a dictionary to store MEG and DNN RDM correlation at each timepoint\n",
    "    for layer in layers:\n",
    "        MEG_correlation[layer] = []\n",
    "\n",
    "    # for loop that goes over MEG RDMs at all time points and correlate with DNN RDMs\n",
    "    for t in range(num_timepoints):\n",
    "        MEG_RDM_t = MEG_RDM_sub_averaged[t, :, :]\n",
    "        for layer in layers:\n",
    "            model_RDM = model_RDMs[layer]\n",
    "            MEG_correlation[layer].append(RSA_spearman(model_RDM, MEG_RDM_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a32719",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Plotting MEG-DNN comparison\n",
    "if run:\n",
    "\n",
    "    plt.rc('font', size=12)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    time_range = range(-100,1201)\n",
    "    ax.plot(time_range, MEG_correlation['layer_1'],\n",
    "            color='tab:orange', label='layer_1')\n",
    "    ax.plot(time_range, MEG_correlation['layer_7'],\n",
    "            color='tab:blue', label='layer_7')\n",
    "\n",
    "    # Same as above\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Spearmans Correlation')\n",
    "    ax.set_title('MEG-model comparison')\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='upper left')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a704837",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Cichy et al., (2014) Resolving human object recognition in space and time. _Nature Neuroscience_ **17**: 455-462. doi: [10.1038/nn.3635](https://doi.org/10.1038/nn.3635)\n",
    "2. Kriegeskorte et al., (2008). Representational similarity analysis â€“ connecting the branches of systems neuroscience. _Frontiers in Systems Neuroscience_ **2**: 4. doi: [10.3389/neuro.06.004.2008](https://doi.org/10.3389/neuro.06.004.2008)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
