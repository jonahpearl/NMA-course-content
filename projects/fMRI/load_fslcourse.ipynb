{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885959fe",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/projects/fMRI/load_fslcourse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/projects/fMRI/load_fslcourse.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41e109",
   "metadata": {},
   "source": [
    "# Load the FSL course task dataset\n",
    "\n",
    "We provide data from 2 Tasks. Please watch the data loading video for more details on the tasks but briefly:\n",
    "\n",
    "- Task 1 : `Fluency`. This is an event related design with three types of events:\n",
    "  - Word-generation events (WG): The subject is presented with a noun, e.g \"car\" and must come up with a pertinent verb (e.g. \"drive\") and then \"think that word in their head\". The subject was explicitly instructed never to say or even mouth a word to prevent movement artefacts.\n",
    "  - Word-shadowing events (WS): The subject is presented with a verb and is instructed to simply \"think that word in their head\".\n",
    "  - Null-events (N): These are events where nothing happens, i.e. the cross-hair remains on the screen and no word is presented. The purpose of these \"events\" is to supply a baseline against which the other two event types can be compared.\n",
    "\n",
    "- Task 2 : `Parametric`. Words were presented at different frequencies. Sentences were presented one word at a time, at frequencies ranging from 50 words per minute (wpm) to 1250 wpm, and the participant just had to read the words as they were presented. This is an example of a parametric experimental design. The hypothesis is that certain brain regions respond more strongly to the optimum reading speed compared to the extremely slow and extremely fast word presentation rates (i.e. you might expect to find an inverted U-shape for the response to the five different levels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3125298",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install nilearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c146d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib   # neuroimaging I/O library\n",
    "\n",
    "from nilearn import plotting, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037854fc",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "DATA_DIR = \"./fslcourse\"\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "# Description of the two experiments\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'fluency' : {\n",
    "     'TR'     : 4.2,         # time resolution in seconds\n",
    "     'ntimes' : 106,         # number of time points\n",
    "     'EVs'    : ['Gen','Shad']  # conditions\n",
    "     },\n",
    "    'parametric' : {\n",
    "     'TR'     : 0.933,\n",
    "     'ntimes' : 1100,\n",
    "     'EVs'    : ['WPM_0050','WPM_0350','WPM_0650','WPM_0950','WPM_1250']\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3629b",
   "metadata": {},
   "source": [
    "# Downloading data\n",
    "\n",
    "The task data are shared in different files, but they will unpack into the same directory structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532eb45",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Download and unzip the data in `DATA_DIR`\n",
    "import os, shutil, requests, tarfile\n",
    "\n",
    "fname = \"fslcourse.tgz\"\n",
    "url = \"https://osf.io/syt65/download/\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            print(\"Downloading data...\")\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)\n",
    "            # open file\n",
    "            with tarfile.open(fname) as fzip:\n",
    "                fzip.extractall(DATA_DIR)\n",
    "            print(\"Download completed!\")\n",
    "else:\n",
    "    print(\"Data have been already downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8495a1f",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "We provide three helper functions:\n",
    "\n",
    "- one for loading the EV file for each task. EV:Explanatory Variable describes the task experiment in terms of stimulus onset, duration, and amplitude.\n",
    "\n",
    "- one for generating a haemodynamic response function. this one is super basic and not at all phyysiological. consider replacing with something that makes more sense.\n",
    "\n",
    "- a function that runs a simple general linear model (or multiple regression of X on Y) and produces t-statistics. It also allows you to generate these statistics for linear combinations of parameter estimates (aka contrasts). This will be handy for the `parametric` data where you can ask whether brain activity changes linearly/quadratically with the stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1615f",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "def load_evs(exp, dir):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Args:\n",
    "      experiment (str) : Name of experiment\n",
    "      dir (str) : Data directory\n",
    "\n",
    "    Returns\n",
    "      evs (dict)\n",
    "\n",
    "    \"\"\"\n",
    "    TR = EXPERIMENTS[exp]['TR']\n",
    "    EVs = []\n",
    "    for ev in EXPERIMENTS[exp]['EVs']:\n",
    "        ev_file = os.path.join(dir, exp, \"EVs\", f\"{ev}.txt\")\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "        a = np.zeros(EXPERIMENTS[exp]['ntimes'])\n",
    "        for frame in frames:\n",
    "            a[frame] = 1\n",
    "        EVs.append(a)\n",
    "    return dict(zip(EXPERIMENTS[exp]['EVs'], EVs))\n",
    "\n",
    "\n",
    "def get_HRF(duration, TR, peak):\n",
    "    \"\"\"\n",
    "    Really dumb Haemodynamic response function (not physiologically plausible)\n",
    "    It simply goes up and down linearly from 0 to peak and back down\n",
    "\n",
    "    Args:\n",
    "      duration (float) : in seconds\n",
    "      TR (float)       : in seconds\n",
    "      peak (float)     : in seconds\n",
    "\n",
    "    Returns:\n",
    "      1D array\n",
    "    \"\"\"\n",
    "    n = int(np.ceil(duration / TR))\n",
    "    x = np.linspace(0, duration, n)\n",
    "    h = np.zeros(n)\n",
    "    h[x < peak]  = x[x < peak] / peak\n",
    "    h[x >= peak] = (x[x >= peak] - duration) / (peak - duration)\n",
    "    h = h / np.sum(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "def glm(Y, X, C=None, mask=None):\n",
    "    \"\"\"\n",
    "    Run a general linear model\n",
    "\n",
    "    Args:\n",
    "      Y (2d array) : time-by-space data matrix\n",
    "      X (2d array) : time-by-regressors design matrix\n",
    "      C (2d array) : contrasts-by-regressor contrrast matrix [default=Identity]\n",
    "      mask (1d array) : spatial mask wherre GLM is run\n",
    "\n",
    "    Returns:\n",
    "      contrast maps\n",
    "      t-stats\n",
    "    \"\"\"\n",
    "    if C is None:\n",
    "        C = np.identity(X.shape[1])\n",
    "    if mask is None:\n",
    "        mask = np.ones(Y.shape[1])\n",
    "\n",
    "    # initialise matrices\n",
    "    beta = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    cope = np.zeros((C.shape[0], Y.shape[1]))\n",
    "    varbeta = np.zeros_like(beta)\n",
    "    tstat = np.zeros_like(beta)\n",
    "\n",
    "    # solve glm\n",
    "    beta[:, mask > 0] = np.linalg.pinv(X) @ Y[:, mask > 0]\n",
    "    # apply contrasts\n",
    "    cope[:, mask > 0] = np.dot(C, beta[:, mask > 0])\n",
    "\n",
    "    # calculate uncertainty (varcope)\n",
    "    r = Y - X@beta\n",
    "    dof = X.shape[0] - np.linalg.matrix_rank(X)\n",
    "    sig2 = np.sum(r**2, axis=0) / dof\n",
    "    varcope = np.outer(C @ np.diag(np.linalg.inv(X.T @ X)) @ C.T, sig2)\n",
    "    # calculate t-stats\n",
    "    tstat[:, mask] = cope[:, mask] / np.sqrt(varcope[:, mask])\n",
    "\n",
    "    return cope, tstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17479bd",
   "metadata": {},
   "source": [
    "# Simple analysis\n",
    "\n",
    "To familiarise ourselves with the data, we will do a simple  analysis where we model the data as a linear combination of two task regressor plus a baseline.\n",
    "\n",
    "We first load the data, load the EVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96171714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nibabel to load the data\n",
    "img = nib.load(os.path.join(DATA_DIR, \"fslcourse_data\",\n",
    "                            \"fluency\", \"fmri_data.nii.gz\"))\n",
    "\n",
    "# get the actual data using the img nibabel Object\n",
    "# this returns a numpy array\n",
    "data = img.get_fdata()\n",
    "\n",
    "# Understand the dimensions of the data\n",
    "print(data.shape)  # x-by-y-by-z-by-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the two EVs\n",
    "plt.figure()\n",
    "plt.plot(load_evs('fluency', os.path.join(DATA_DIR, \"fslcourse_data\"))['Gen'],\n",
    "                  label='Word Generation')\n",
    "plt.plot(load_evs('fluency', os.path.join(DATA_DIR, \"fslcourse_data\"))['Shad'],\n",
    "         label='Word Shadowing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784ed9",
   "metadata": {},
   "source": [
    "Next we will convolve ouur regressors with the HRF. This is because the FMRI signal is a sluggish blood signal that lags behind neural signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HRF = get_HRF(duration=10, TR=1, peak=3)\n",
    "EVs = load_evs('fluency', os.path.join(DATA_DIR, \"fslcourse_data\"))\n",
    "n = len(EVs['Gen'])\n",
    "ev1 = np.convolve(EVs['Gen'], HRF, 'full')[:n]\n",
    "ev2 = np.convolve(EVs['Shad'], HRF, 'full')[:n]\n",
    "\n",
    "# plot the new EVs:\n",
    "plt.figure()\n",
    "plt.plot(EVs['Gen'])\n",
    "plt.plot(ev1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(EVs['Shad'])\n",
    "plt.plot(ev2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000dd0b",
   "metadata": {},
   "source": [
    "Next we create the design matrix to use in the GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f35221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demean the task regressors\n",
    "ev1 = ev1 - np.mean(ev1)\n",
    "ev2 = ev2 - np.mean(ev2)\n",
    "\n",
    "# append a constant regressor\n",
    "design_matrix = np.asarray([ev1, ev2, np.ones_like(ev1)]).T\n",
    "print(design_matrix.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(design_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcdcbe",
   "metadata": {},
   "source": [
    "Next we do the GLM. We provide a helper function to calcculate a T-statistic for each of the regression coefficients by dividing eacch coefficient by its uncertainty (varbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn 4D data array to 2D (N-by-time)\n",
    "Y = np.reshape(data, (np.prod(data.shape[:3]), -1)).T\n",
    "\n",
    "# create a mask where data is non-zero\n",
    "mask = np.sum(Y**2, axis=0)>0\n",
    "\n",
    "# run GLM\n",
    "beta, t = glm(Y=Y, X=design_matrix, mask=mask)\n",
    "\n",
    "# turn back to 4D arrays\n",
    "shape = list(data.shape[:3])\n",
    "shape.append(beta.shape[0])\n",
    "beta_r = np.reshape(beta.T,shape)\n",
    "t_r = np.reshape(t.T,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at fit for best voxel\n",
    "idx = np.argmax(t[0, :])\n",
    "print(t[:, idx])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Y[:, idx])\n",
    "plt.plot(design_matrix @ beta[:, idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1ee14",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "We will use nilearn to visualise the GLM results on a brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd380d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the nilearn package\n",
    "\n",
    "# select Word Gen contrast\n",
    "tt  = t_r[..., 0]\n",
    "\n",
    "# turn into nilearn image object\n",
    "map = image.new_img_like(img, tt, copy_header=True)\n",
    "\n",
    "# use to display (assumes that the map is in MNI standard space, which it is because the data is)\n",
    "plotting.plot_stat_map(map, threshold=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cc00f",
   "metadata": {},
   "source": [
    "You should be able to see that, despite using such a cheap and cheerful model, we can see activity in Broca's area. Woohoo!\n",
    "\n",
    "\n",
    "That's it for now. You can try different HRF models, or get the parametric data and try to see if you can model U-shaped brain responses with the appropriate contrast matrix.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
